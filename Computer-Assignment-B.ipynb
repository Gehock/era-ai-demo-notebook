{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Assignment B\n",
    "\n",
    "## Reinforcement learning for Tic-Tac-Toe\n",
    "\n",
    "In this exercise, you get to train a game-playing AI from scratch for the classic game of Tic-Tac-Toe (also known as Noughts and Crosses). This exercise kind of assumes that you are familiar with the game and if youo are not, please read the Wikipedia article [1], consult your friends, or challenge the course assistant to play against you with pen and paper. We consider the simplest version of the game, where only a 3x3 grid is used.\n",
    "\n",
    "We will consider Q-learning, a model-free reinforcement learning algorithm. The goal of Q-learning is to learn a policy (what to do), which tells an agent what action to take under what circumstances (depending on where the opponent puts their mark). It does not require a model (thus called 'model-free') of the environment, and it can learn purely by self-play once it has been given the rules of the game. The agent will receive a reward of `+1` if it wins and `-1` if it loses a game. \n",
    "\n",
    "This time, no data is loaded and you also need to start by training the model (remember that last time you skipped training). We use a modified implementation following [2], and all code needed for this task is presented below.\n",
    "\n",
    "[1] Tic-tac-toe. In Wikipedia, The Free Encyclopedia. URL: https://en.wikipedia.org/wiki/Tic-tac-toe\n",
    "\n",
    "[2] Neil Slater (2017). Game Playing Script(s). URL: https://github.com/neilslater/game_playing_scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Setup methods\n",
    "Run the code below to set up our Q-learning framework for Tic-Tac-Toe.\n",
    "### Utility Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "   Copyright 2017 Neil Slater\n",
    "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "   you may not use this file except in compliance with the License.\n",
    "   You may obtain a copy of the License at\n",
    "       http://www.apache.org/licenses/LICENSE-2.0\n",
    "   Unless required by applicable law or agreed to in writing, software\n",
    "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "   See the License for the specific language governing permissions and\n",
    "   limitations under the License.\n",
    "   \n",
    "   The original code as been modified for educatonal purposes.\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class TicTacToeGame():\n",
    "    def __init__(self):\n",
    "        self.state = '         '  #a string of length 9 that encode the state of the 3*3 board \n",
    "        self.player = 'X'\n",
    "        self.winner = None\n",
    "\n",
    "    def allowed_moves(self):      #find the empty position in the state string \n",
    "        states = []               #store all possible next states\n",
    "        for i in range(len(self.state)):\n",
    "            if self.state[i] == ' ':\n",
    "                states.append(self.state[:i] + self.player + self.state[i+1:]) \n",
    "        return states\n",
    "\n",
    "    def make_move(self, next_state):\n",
    "        if self.winner:\n",
    "            raise(Exception(\"Game already completed, cannot make another move!\"))\n",
    "        if not self.__valid_move(next_state):\n",
    "            raise(Exception(\"Cannot make move {} to {} for player {}\".format(\n",
    "                    self.state, next_state, self.player)))\n",
    "\n",
    "        self.state = next_state\n",
    "        self.winner = self.predict_winner(self.state)\n",
    "        if self.winner:\n",
    "            self.player = None\n",
    "        elif self.player == 'X':\n",
    "            self.player = 'O'\n",
    "        else:\n",
    "            self.player = 'X'\n",
    "\n",
    "    def playable(self):\n",
    "        return ( (not self.winner) and any(self.allowed_moves()) )\n",
    "\n",
    "    def predict_winner(self, state):\n",
    "        lines = [(0,1,2), (3,4,5), (6,7,8), (0,3,6), (1,4,7), (2,5,8), (0,4,8), (2,4,6)]  # all possible lines\n",
    "        winner = None\n",
    "        for line in lines:\n",
    "            line_state = state[line[0]] + state[line[1]] + state[line[2]]\n",
    "            if line_state == 'XXX':\n",
    "                winner = 'X'\n",
    "            elif line_state == 'OOO':\n",
    "                winner = 'O'\n",
    "        return winner\n",
    "\n",
    "    def __valid_move(self, next_state):\n",
    "        allowed_moves = self.allowed_moves()  #get all possible next states\n",
    "        if any(state == next_state for state in allowed_moves): #check if the input next_state is in \n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def print_board(self):\n",
    "        s = self.state\n",
    "        print('     {} | {} | {} '.format(s[0],s[1],s[2]))\n",
    "        print('    -----------')\n",
    "        print('     {} | {} | {} '.format(s[3],s[4],s[5]))\n",
    "        print('    -----------')\n",
    "        print('     {} | {} | {} '.format(s[6],s[7],s[8]))\n",
    "\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, game_class, epsilon=0.1, alpha=0.5, value_player='X'):\n",
    "        self.V = dict()  #dictionary to store value\n",
    "        self.NewGame = game_class\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.value_player = value_player\n",
    "\n",
    "    def state_value(self, game_state):\n",
    "        return self.V.get(game_state, 0.0)\n",
    "\n",
    "    def learn_game(self, num_episodes=1000):\n",
    "        for episode in range(num_episodes):\n",
    "            self.learn_from_episode()\n",
    "\n",
    "    def learn_from_episode(self):\n",
    "        game = self.NewGame()\n",
    "        _, move = self.learn_select_move(game)\n",
    "        while move:\n",
    "            move = self.learn_from_move(game, move)\n",
    "\n",
    "\n",
    "\n",
    "    def learn_from_move(self, game, move):\n",
    "        game.make_move(move)\n",
    "        r = self.__reward(game)   \n",
    "        td_target = r\n",
    "        next_state_value = 0.0\n",
    "        selected_next_move = None\n",
    "        if game.playable():\n",
    "            best_next_move, selected_next_move = self.learn_select_move(game)\n",
    "            next_state_value = self.state_value(best_next_move)\n",
    "        current_state_value = self.state_value(move)\n",
    "        td_target = r + next_state_value\n",
    "        self.V[move] = current_state_value + self.alpha * (td_target - current_state_value)  #update value \n",
    "        return selected_next_move\n",
    "\n",
    "    def learn_select_move(self, game):\n",
    "        allowed_state_values = self.__state_values( game.allowed_moves() )\n",
    "        if game.player == self.value_player:\n",
    "            best_move = self.__argmax_V(allowed_state_values)\n",
    "        else:\n",
    "            best_move = self.__argmin_V(allowed_state_values)\n",
    "\n",
    "        selected_move = best_move\n",
    "        if random.random() < self.epsilon:  #epsilon-greedy strategyâ€Š\n",
    "            selected_move = self.__random_V(allowed_state_values)\n",
    "\n",
    "        return (best_move, selected_move)\n",
    "\n",
    "    def play_select_move(self, game):\n",
    "        allowed_state_values = self.__state_values( game.allowed_moves() )\n",
    "        if game.player == self.value_player:\n",
    "            return self.__argmax_V(allowed_state_values)\n",
    "        else:\n",
    "            #return self.__argmin_V(allowed_state_values)\n",
    "            return self.__random_V(allowed_state_values)\n",
    "\n",
    "    def demo_game(self, verbose=False):\n",
    "        game = self.NewGame()\n",
    "        t = 0\n",
    "        while game.playable():\n",
    "            if verbose:\n",
    "                print(\" \\nTurn {}\\n\".format(t))\n",
    "                game.print_board()\n",
    "            move = self.play_select_move(game)\n",
    "            game.make_move(move)\n",
    "            t += 1\n",
    "        if verbose:\n",
    "            print(\" \\nTurn {}\\n\".format(t))\n",
    "            game.print_board()\n",
    "        if game.winner:\n",
    "            if verbose:\n",
    "                print(\"\\n{} is the winner!\".format(game.winner))\n",
    "            return game.winner\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"\\nIt's a draw!\")\n",
    "            return '-'\n",
    "\n",
    "    def interactive_game(self, agent_player='X'):\n",
    "        game = self.NewGame()\n",
    "        t = 0\n",
    "        while game.playable():\n",
    "            print(\" \\nTurn {}\\n\".format(t))\n",
    "            game.print_board()\n",
    "            if game.player == agent_player:\n",
    "                move = self.play_select_move(game)\n",
    "                game.make_move(move)\n",
    "            else:\n",
    "                move = self.__request_human_move(game)\n",
    "                game.make_move(move)\n",
    "            t += 1\n",
    "\n",
    "        print(\" \\nTurn {}\\n\".format(t))\n",
    "        game.print_board()\n",
    "\n",
    "        if game.winner:\n",
    "            print(\"\\n{} is the winner!\".format(game.winner))\n",
    "            return game.winner\n",
    "        print(\"\\nIt's a draw!\")\n",
    "        return '-'\n",
    "\n",
    "    def round_V(self):\n",
    "        # After training, this makes action selection random from equally-good choices\n",
    "        for k in self.V.keys():\n",
    "            self.V[k] = round(self.V[k],1)\n",
    "\n",
    "\n",
    "    def __state_values(self, game_states):\n",
    "        return dict((state, self.state_value(state)) for state in game_states)\n",
    "\n",
    "    def __argmax_V(self, state_values):\n",
    "        max_V = max(state_values.values())\n",
    "        chosen_state = random.choice([state for state, v in state_values.items() if v == max_V])\n",
    "        return chosen_state\n",
    "\n",
    "    def __argmin_V(self, state_values):\n",
    "        min_V = min(state_values.values())\n",
    "        chosen_state = random.choice([state for state, v in state_values.items() if v == min_V])\n",
    "        return chosen_state\n",
    "\n",
    "    def __random_V(self, state_values):\n",
    "        return random.choice(list(state_values.keys()))\n",
    "\n",
    "    def __reward(self, game):\n",
    "        if game.winner == self.value_player:\n",
    "            return 1.0\n",
    "        elif game.winner:\n",
    "            return -1.0\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "    def __request_human_move(self, game):\n",
    "        allowed_moves = [i+1 for i in range(9) if game.state[i] == ' ']\n",
    "        human_move = None\n",
    "        while not human_move:\n",
    "            idx = int(input('Choose move for {}, from {} : '.format(game.player, allowed_moves)))\n",
    "            if any([i==idx for i in allowed_moves]):\n",
    "                human_move = game.state[:idx-1] + game.player + game.state[idx:]\n",
    "        return human_move\n",
    "\n",
    "def demo_game_stats(agent):\n",
    "    results = [agent.demo_game() for i in range(10000)]\n",
    "    game_stats = {k: results.count(k)/100 for k in ['X', 'O', '-']}\n",
    "    print(\"    percentage results: {}\".format(game_stats))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Make an agent\n",
    "Now it is time to create you game-playing AI agent. Here you choose name for it (O/X, note that X always starts) and set the learning-rate parameter epsilon and discount factor for discounting future rewards. We also print out some test stats of the efficiency of this AI before learning (NB: '`-`' stands for a draw)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create you agent (named 'X') and assign the learning rate and discount factor\n",
    "agent = Agent(TicTacToeGame, epsilon = 0.1, alpha = 0.5, value_player='X')\n",
    "\n",
    "print(\"Before learning:\")\n",
    "demo_game_stats(agent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Play against your agent\n",
    "Try out how well your agent works. You can play interactive games against the agent by running the cell below. The AI will ask you for your next move and then proceed to make its move. The moves are given by indicating which cell you want to put your mark in:\n",
    "```\n",
    "     1 | 2 | 3 \n",
    "    -----------\n",
    "     4 | 5 | 6 \n",
    "    -----------\n",
    "     7 | 8 | 9 \n",
    " ```\n",
    " Play a couple of games (you should be able to win quite easily...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.interactive_game()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4. Train the agent through self-play\n",
    "You can have the agent play against itself for a number of games in order to learn how to play the game. In the cell below you will make the agent play 1000 games. After that, try playing against it again. Can you see any difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.learn_game(1000)\n",
    "print(\"After 1000 learning games:\")\n",
    "demo_game_stats(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.interactive_game()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5. More training and playing\n",
    "Continue training the method:\n",
    "* First for 4000 more games, then try playing again. \n",
    "* Finally, keep training the model for 10000 games more and try playing again.\n",
    "\n",
    "What do you notice? Is the agent getting harder to beat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.interactive_game()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Tweak the model\n",
    "Now you can tweak the model, and experiment further:\n",
    "* Start by changing the AI identity to \"O\" (so that you get to start the games). Re-train and re-try playing.\n",
    "* You can also experiment with changing the learning rate and discounting factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
